---
title: "DATA622 Assignment 1"
author: "Banu & Lucas"
date: "10/10/2024"
toc: true
format:
  html:
    html-math-method: katex
    code-tools: true
    self-contained: true
    toc_depth: 2
execute:
  warning: false
---

```{r}
#Load Packages
library(arrow)
library(dplyr)
library(ggplot2)
library(reshape2)
library(patchwork)
library(lubridate)
```

## Project

### **Deliverables**

Explore how to analyze and predict an outcome based on the data available. This will be an exploratory exercise, so feel free to show errors and warnings that raise during the analysis. Test the code with both datasets selected and compare the results.

### **Goals**

1.  Are the columns of your data correlated?

2.  Are there labels in your data? Did that impact your choice of algorithm?

3.  What are the pros and cons of each algorithm you selected?

4.  How your choice of algorithm relates to the datasets (was your choice of algorithm impacted by the datasets you chose)?

5.  Which result will you trust if you need to make a business decision?

6.  Do you think an analysis could be prone to errors when using too much data, or when using the least amount possible?

7.  How does the analysis between data sets compare?

## Large Data Set (End to End ML Analysis)

### Data Set Introduction

The large data set consists of about 284,000 card transactions that are labelled as non-fraud and fraud. It is a real data set from a European financial institution, which is why the features are masked. They are the result of extensive PCA. Additionally, it is a highly imbalanced data set, as there are several orders of magnitude more non-fraud than fraud transactions.

### Data Exploration & Plots

```{r}
set.seed(2024)
path = "https://github.com/BanuB/Card_Transaction_Fraud/raw/refs/heads/master/creditcard.parquet" 
#path = "C:\\Users\\f9tbqno\\Downloads\\creditcard.parquet"
#Set path of Parquet File downloaded from Github repo: https://github.com/lucasweyrich958/DATA622/blob/main/sales.parquet

tx_raw = read_parquet(path)
```

```{r}
summary(tx_raw)
tx_raw$Class = as.factor(tx_raw$Class) #Convert Class column to factor

tx_raw = tx_raw %>%
  mutate(datetime = as.POSIXct("2024-01-01 00:00:00", tz = "UTC") + seconds(Time)) #Make new column that shows datetime

ggplot(tx_raw, aes(x = Amount, fill = Class)) +
  geom_histogram(position = "dodge", bins = 60) +
  labs(title = "Histogram of Amounts by Class (< 500 USD)", x = "Amount (USD)", y = "Frequency") +
  theme_minimal() +
  scale_fill_manual(values = c('grey', 'green')) +
  xlim(0, 500)

tx_1 = tx_raw %>%
  filter(Class == 1)

ggplot(tx_1, aes(x = Amount)) +
  geom_histogram(position = "dodge", bins = 60) +
  labs(title = "Histogram of Amounts for Class Fraud", x = "Amount (USD)", y = "Frequency") +
  theme_minimal()

#Correlation Heatmap
tx_raw_numeric = tx_raw %>%
  dplyr::select(!c(Class, datetime))
cor_matrix = cor(tx_raw_numeric)
cor_matrix = melt(cor_matrix)

ggplot(data = cor_matrix, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), space = "Lab", 
                       name = "Correlation") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 10, hjust = 1)) +
  coord_fixed() +
  labs(title = "Correlation Heatmap", x = "Variable", y = "Variable")

#----Time-Series for Transactions----
tx_transactions <- tx_raw %>%
  mutate(datetime_hour = floor_date(datetime, "hour")) %>%
  group_by(datetime_hour, Class) %>%
  summarise(transaction_count = n())

tx_trans_1 <- ggplot(tx_transactions, aes(x = datetime_hour, y = transaction_count, color = as.factor(Class))) +
  geom_line() +
  theme_minimal() +
  labs(title = 'Fraud Txs', y = "Number of Transactions", x = "Time (Hourly)") +
  scale_y_continuous(limits = c(0, 50)) +
  theme(legend.position = "none") +
  annotate("text", x = max(tx_transactions$datetime_hour), y = 45, label = expression(rho[1] == -0.226), hjust = 1)

tx_trans_0 <- ggplot(tx_transactions, aes(x = datetime_hour, y = transaction_count, color = as.factor(Class))) +
  geom_line() +
  theme_minimal() +
  labs(title = 'Non-Fraud Txs', x = NULL, y = NULL, color = "Class") +
  scale_y_continuous(limits = c(1000, max(tx_transactions$transaction_count))) +
  theme(legend.position = "none") +
  annotate("text", x = max(tx_transactions$datetime_hour), y = max(tx_transactions$transaction_count) - 50, label = expression(rho[1] == 0.918), hjust = 1)

# Combine the two plots
tx_transactions_plot <- (tx_trans_0 / tx_trans_1) + plot_layout(heights = c(2, 1))
print(tx_transactions_plot)

#----Auto- and Cross-correlations----
tx_nofraud = tx_transactions %>%
  filter(Class == 0) %>%
  dplyr::select(transaction_count)
tx_nofraud = tx_nofraud$transaction_count

tx_nofraud_autocor = acf(tx_nofraud, lag.max = 3, plot = T)

tx_fraud = tx_transactions %>%
  filter(Class == 1) %>%
  dplyr::select(transaction_count)
tx_fraud = tx_fraud$transaction_count

tx_fraud_autocor = acf(tx_fraud, lag.max =3, plot = F)

tx_nofraud_autocor
tx_fraud_autocor


```

### Data Preparation

```{r}
set.seed(2024)
library(caret)
library(e1071)
library(randomForest)
library(rpart)
library(pROC)
library(ranger)
library(ranger)

tx_raw$Class = as.factor(tx_raw$Class)

# Ensure datetime is of the correct type
tx_raw$datetime = as.POSIXct(tx_raw$datetime)

# Split the data into training and testing sets
trainIndex = createDataPartition(tx_raw$Class, p = 0.8, list = FALSE)
dataTrain = tx_raw[trainIndex, ]
dataTest = tx_raw[-trainIndex, ]

#Define CV
train_control = trainControl(method = "cv", number = 10)
```

### Algorithm Selection

Given the fact the this is a highly imbalanced data set, a weak learner, such as a decision tree or logistic regression will likely not be very successful. Therefore, the better choice will likely be an ensemble. In order to test this, we will run a logistic regression, a single decision tree, a random forest and a XGBoost tree. Then, we will compare measures for accuracy to understand the differences.

#### Train Models

```{r}
set.seed(2024)
#Logistic Regression
time_logistic_train = system.time({
  logistic_model = train(Class ~ ., data = dataTrain, method = "glm", family = "binomial", trControl = train_control)})

#Decision Tree
time_tree_train = system.time({
  tree_model = train(Class ~ ., data = dataTrain, method = "rpart", trControl = train_control)})
```

#### Predictions and Evaluation

```{r}
#Logistic Regression Prediction
time_logistic_pred = system.time({
  logistic_pred = predict(logistic_model, dataTest)
  logistic_probs = predict(logistic_model, dataTest, type = "prob")[, 2]
})

#Decision Tree Prediction
time_tree_pred = system.time({
  tree_pred = predict(tree_model, dataTest)
  tree_probs = predict(tree_model, dataTest, type = "prob")[, 2]
})

# Logistic Regression Confusion Matrix
confusionMatrix(logistic_pred, dataTest$Class)

# Decision Tree Confusion Matrix
confusionMatrix(tree_pred, dataTest$Class)
```

```{r}
#Benchmarking Training and Prediction Time
benchmark_results = data.frame(
  Model = c("Logistic Regression", "Decision Tree"),
  Training_Time = c(time_logistic_train[3], time_tree_train[3]),
  Prediction_Time = c(time_logistic_pred[3], time_tree_pred[3])
)

print(benchmark_results)
```

```{r}
#ROC and AUC Curves
#ROC
roc_logistic = roc(dataTest$Class, logistic_probs)
roc_tree = roc(dataTest$Class, tree_probs)

plot(roc_logistic, col = "red", main = "ROC Curves", lwd = 2)
lines(roc_tree, col = "blue", lwd = 2)
legend("bottomright", legend = c("Logistic Regression", "Decision Tree"),
       col = c("red", "blue"), lwd = 2)

#AUC
auc_logistic = auc(roc_logistic)
auc_tree = auc(roc_tree)

print(paste("AUC for Logistic Regression:", auc_logistic))
print(paste("AUC for Decision Tree:", auc_tree))
```

### Large Dataset Essay Summary

## Small DataSet (End to End ML Analysis)

### Dataset Introduction

### Data Exploration & Plots

### Data Preparation

### Algorithm Selection/Build Models

### Large DataSet Essay Summary

## Joint Comparison and Summary Essay
