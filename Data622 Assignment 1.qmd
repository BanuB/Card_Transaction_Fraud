---
title: "DATA622 Assignment 1"
author: "Banu & Lucas"
date: "10/10/2024"
toc: true
format:
  html:
    html-math-method: katex
    code-tools: true
    self-contained: true
    toc_depth: 2
execute:
  warning: false
---

```{r}
#Load Packages
library(arrow)
library(dplyr)
library(ggplot2)
library(reshape2)
library(patchwork)
library(lubridate)
library(rpart.plot)
```

## Project

### **Deliverables**

Explore how to analyze and predict an outcome based on the data available. This will be an exploratory exercise, so feel free to show errors and warnings that raise during the analysis. Test the code with both datasets selected and compare the results.

### **Goals**

1.  Are the columns of your data correlated?

2.  Are there labels in your data? Did that impact your choice of algorithm?

3.  What are the pros and cons of each algorithm you selected?

4.  How your choice of algorithm relates to the datasets (was your choice of algorithm impacted by the datasets you chose)?

5.  Which result will you trust if you need to make a business decision?

6.  Do you think an analysis could be prone to errors when using too much data, or when using the least amount possible?

7.  How does the analysis between data sets compare?

## Large Data Set (End to End ML Analysis)

### Data Set Introduction

The large data set consists of about 284,000 card transactions that are labelled as non-fraud and fraud. It is a real data set from a European financial institution, which is why the features are masked. They are the result of extensive PCA. Additionally, it is a highly imbalanced data set, as there are several orders of magnitude more non-fraud than fraud transactions.

### Data Exploration & Plots

```{r}
set.seed(2024)
path = "https://github.com/BanuB/Card_Transaction_Fraud/raw/refs/heads/master/creditcard.parquet" 

tx_raw = read_parquet(path)
```

```{r}
summary(tx_raw)
tx_raw$Class = as.factor(tx_raw$Class) #Convert Class column to factor

tx_raw = tx_raw %>%
  mutate(datetime = as.POSIXct("2024-01-01 00:00:00", tz = "UTC") + seconds(Time)) #Make new column that shows datetime

ggplot(tx_raw, aes(x = Amount, fill = Class)) +
  geom_histogram(position = "dodge", bins = 60) +
  labs(title = "Histogram of Amounts by Class (< 500 USD)", x = "Amount (USD)", y = "Frequency") +
  theme_minimal() +
  scale_fill_manual(values = c('grey', 'green')) +
  xlim(0, 500)

tx_1 = tx_raw %>%
  filter(Class == 1)

ggplot(tx_1, aes(x = Amount)) +
  geom_histogram(position = "dodge", bins = 60) +
  labs(title = "Histogram of Amounts for Class Fraud", x = "Amount (USD)", y = "Frequency") +
  theme_minimal()

#Correlation Heatmap
tx_raw_numeric = tx_raw %>%
  dplyr::select(!c(Class, datetime))
cor_matrix = cor(tx_raw_numeric)
cor_matrix = melt(cor_matrix)

ggplot(data = cor_matrix, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), space = "Lab", 
                       name = "Correlation") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 10, hjust = 1)) +
  coord_fixed() +
  labs(title = "Correlation Heatmap", x = "Variable", y = "Variable")

#----Time-Series for Transactions----
tx_transactions <- tx_raw %>%
  mutate(datetime_hour = floor_date(datetime, "hour")) %>%
  group_by(datetime_hour, Class) %>%
  summarise(transaction_count = n())

tx_trans_1 <- ggplot(tx_transactions, aes(x = datetime_hour, y = transaction_count, color = as.factor(Class))) +
  geom_line() +
  theme_minimal() +
  labs(title = 'Fraud Txs', y = "Number of Transactions", x = "Time (Hourly)") +
  scale_y_continuous(limits = c(0, 50)) +
  theme(legend.position = "none") +
  annotate("text", x = max(tx_transactions$datetime_hour), y = 45, label = expression(rho[1] == -0.226), hjust = 1)

tx_trans_0 <- ggplot(tx_transactions, aes(x = datetime_hour, y = transaction_count, color = as.factor(Class))) +
  geom_line() +
  theme_minimal() +
  labs(title = 'Non-Fraud Txs', x = NULL, y = NULL, color = "Class") +
  scale_y_continuous(limits = c(1000, max(tx_transactions$transaction_count))) +
  theme(legend.position = "none") +
  annotate("text", x = max(tx_transactions$datetime_hour), y = max(tx_transactions$transaction_count) - 50, label = expression(rho[1] == 0.918), hjust = 1)

# Combine the two plots
tx_transactions_plot <- (tx_trans_0 / tx_trans_1) + plot_layout(heights = c(2, 1))
print(tx_transactions_plot)

#----Auto- and Cross-correlations----
tx_nofraud = tx_transactions %>%
  filter(Class == 0) %>%
  dplyr::select(transaction_count)
tx_nofraud = tx_nofraud$transaction_count

tx_nofraud_autocor = acf(tx_nofraud, lag.max = 3, plot = T)

tx_nofraud_autocor


```

When investigating the plots from the EDA above one thing becomes clear: the data set is HEAVILY imbalanced. As discussed in the introduction above, this is unsurprising given the nature of non-fraud versus fraud transactions; however, this is an important consideration when selecting the models to run. Weak learners will likely not be as strong in performance as ensemble methods would be.

Additionally, there are a few more interesting observations. For example, the correlation matrix between all features show no strong correlation between each other. This is important for several machine learning algorithms, and considering that this data set has undergone feature engineering and PCA, it is unsurprising that this is case. Nevertheless, this plot should be part of any machine learning implementation.

Looking at the time-series graph, plotting the amounts of transactions per hour, over the time span of the data set, the cyclic nature of the non-fraud transactions is very apparent. This is not existent in the fraud transactions, which are mostly randomly happening. This can also be observed in the auto-correlations: &rho; for the non-fraud transactions is 0.92, which points to a strong predictability for the next data point (i.e., after an increase in count, another increase if followed). The negative &rho; of -0.23 of the fraudulent transactions points to a more random behavior across these two and a half days of time period of the data set. This feature will surely be quite important for the algorithm during training.

Lastly, these auto-correlations can be seen in the ACF plot, that shows different lags. It can be seen that the strongest lag is 1, with decreasing auto-correlations with larger lags.

Next, we split the data to prepare for the machine learning implementation. We chose to split the data 80/20 for training and test set. We deferred from a validation set as we are not going to engage in hyper parameter tuning in this exercise.

### Data Preparation

```{r}
set.seed(2024)
library(caret)
library(e1071)
library(randomForest)
library(rpart)
library(pROC)
library(ranger)
library(ranger)

tx_raw$Class = as.factor(tx_raw$Class)

# Ensure datetime is of the correct type
tx_raw$datetime = as.POSIXct(tx_raw$datetime)

# Split the data into training and testing sets
trainIndex = createDataPartition(tx_raw$Class, p = 0.8, list = FALSE)
dataTrain = tx_raw[trainIndex, ]
dataTest = tx_raw[-trainIndex, ]

#Define CV
train_control = trainControl(method = "cv", number = 10)
```

### Algorithm Selection

Given the fact the this is a highly imbalanced data set, a weak learner, such as a decision tree or logistic regression will likely not be very successful. Therefore, the better choice will likely be an ensemble. In order to test this, we will run a logistic regression, and a single decision tree. We wanted to also include a random forest, however, the large data set was computationally too expensive. In the real world, we would certainly use some type of ensemble method, like random forest and XGBoost.

#### Train Models

```{r}
set.seed(2024)
#Logistic Regression
time_logistic_train = system.time({
  logistic_model = train(Class ~ ., data = dataTrain, method = "glm", family = "binomial", trControl = train_control)})

#Decision Tree
time_tree_train = system.time({
  tree_model = train(Class ~ ., data = dataTrain, method = "rpart", trControl = train_control)})
```

#### Predictions and Evaluation

```{r}
#Logistic Regression Prediction
time_logistic_pred = system.time({
  logistic_pred = predict(logistic_model, dataTest)
  logistic_probs = predict(logistic_model, dataTest, type = "prob")[, 2]
})

#Decision Tree Prediction
time_tree_pred = system.time({
  tree_pred = predict(tree_model, dataTest)
  tree_probs = predict(tree_model, dataTest, type = "prob")[, 2]
})

# Logistic Regression Confusion Matrix
confusionMatrix(logistic_pred, dataTest$Class)

# Decision Tree Confusion Matrix
confusionMatrix(tree_pred, dataTest$Class)
```

```{r}
#Benchmarking Training and Prediction Time
benchmark_results = data.frame(
  Model = c("Logistic Regression", "Decision Tree"),
  Training_Time = c(time_logistic_train[3], time_tree_train[3]),
  Prediction_Time = c(time_logistic_pred[3], time_tree_pred[3])
)

print(benchmark_results)
```

The performance of both, the logistic regression and the decision tree are good, with above 90% accuracy. Looking at the timing benchmarks, both models trained within about one minute, and took only seconds to predict the test set of 50,000 rows. As mentioned above, the random forest trained much longer, on the magnitude of hours, so we chose to not continue with this at this time.

```{r}
#ROC and AUC Curves
#ROC

roc_logistic = roc(dataTest$Class, logistic_probs)
roc_tree = roc(dataTest$Class, tree_probs)

plot(roc_logistic, col = "red", main = "ROC Curves", lwd = 2)
lines(roc_tree, col = "blue", lwd = 2)
legend("bottomright", legend = c("Logistic Regression", "Decision Tree"),
       col = c("red", "blue"), lwd = 2)

#AUC
auc_logistic = auc(roc_logistic)
auc_tree = auc(roc_tree)

print(paste("AUC for Logistic Regression:", auc_logistic))
print(paste("AUC for Decision Tree:", auc_tree))

coefficients <- tidy(logistic_model$finalModel)
coefficients <- coefficients[coefficients$term != "(Intercept)", ]  # Remove intercept for better visualization
ggplot(coefficients, aes(x = reorder(term, estimate), y = estimate)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Logistic Regression Coefficients", x = "Features", y = "Coefficient") +
  theme_minimal()

rpart.plot(tree_model$finalModel)
```

In order to further understand the predictive quality of both models, we decided to compute ROC curves that plot specificity against sensitivity, and here found, interestingly, that the logistic regression was much better across the board than the decision tree. It is likely that the tree over fit, which leads to diminished predictive quality. While the accuracy is still high, this does not mean that it stays high with other unseen data.

Therefore, in the current case, we'd be deciding to utilize logistic regression over the decision tree. While again, an ensemble of trees would likely outperform the logistic regression, even with a smaller data set.

### Large Dataset Essay Summary

## Small DataSet (End to End ML Analysis)

### Dataset Introduction

### Data Exploration & Plots

### Data Preparation

### Algorithm Selection/Build Models

### Large DataSet Essay Summary

## Joint Comparison and Summary Essay
